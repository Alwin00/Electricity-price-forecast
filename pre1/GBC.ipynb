{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 3) (4,) (1, 3) (1, 1)\n",
      "1 [1]\n",
      "[0.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "'''\n",
    "调参：\n",
    "loss：损失函数。有deviance和exponential两种。deviance是采用对数似然，exponential是指数损失，后者相当于AdaBoost。\n",
    "n_estimators:最大弱学习器个数，默认是100，调参时要注意过拟合或欠拟合，一般和learning_rate一起考虑。\n",
    "learning_rate:步长，即每个弱学习器的权重缩减系数，默认为0.1，取值范围0-1，当取值为1时，相当于权重不缩减。较小的learning_rate相当于更多的迭代次数。\n",
    "subsample:子采样，默认为1，取值范围(0,1]，当取值为1时，相当于没有采样。小于1时，即进行采样，按比例采样得到的样本去构建弱学习器。这样做可以防止过拟合，但是值不能太低，会造成高方差。\n",
    "init：初始化弱学习器。不使用的话就是第一轮迭代构建的弱学习器.如果没有先验的话就可以不用管\n",
    "\n",
    "由于GBDT使用CART回归决策树。以下参数用于调优弱学习器，主要都是为了防止过拟合\n",
    "max_feature：树分裂时考虑的最大特征数，默认为None，也就是考虑所有特征。可以取值有：log2,auto,sqrt\n",
    "max_depth：CART最大深度，默认为None\n",
    "min_sample_split：划分节点时需要保留的样本数。当某节点的样本数小于某个值时，就当做叶子节点，不允许再分裂。默认是2\n",
    "min_sample_leaf：叶子节点最少样本数。如果某个叶子节点数量少于某个值，会同它的兄弟节点一起被剪枝。默认是1\n",
    "min_weight_fraction_leaf：叶子节点最小的样本权重和。如果小于某个值，会同它的兄弟节点一起被剪枝。一般用于权重变化的样本。默认是0\n",
    "min_leaf_nodes：最大叶子节点数\n",
    "'''\n",
    "\n",
    "gbdt = GradientBoostingClassifier(loss='deviance', learning_rate=0.1, n_estimators=5, subsample=1\n",
    "                                  , min_samples_split=2, min_samples_leaf=1, max_depth=3\n",
    "                                  , init=None, random_state=None, max_features=None\n",
    "                                  , verbose=0, max_leaf_nodes=None, warm_start=False\n",
    "                                  )\n",
    "\n",
    "train_feat = np.array([[1, 5, 20],\n",
    "                       [2, 7, 30],\n",
    "                       [3, 21, 70],\n",
    "                       [4, 30, 60],\n",
    "                       ])\n",
    "train_label = np.array([[0], [0], [1], [1]]).ravel()\n",
    "\n",
    "test_feat = np.array([[5, 25, 65]])\n",
    "test_label = np.array([[1]])\n",
    "print(train_feat.shape, train_label.shape, test_feat.shape, test_label.shape)\n",
    "\n",
    "gbdt.fit(train_feat, train_label)\n",
    "pred = gbdt.predict(test_feat)\n",
    "\n",
    "total_err = 0\n",
    "for i in range(pred.shape[0]):\n",
    "    print(pred[i], test_label[i])\n",
    "    err = (pred[i] - test_label[i]) / test_label[i]\n",
    "    total_err += err * err\n",
    "print(total_err / pred.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gbdt = GradientBoostingClassifier(loss='deviance', learning_rate=0.1, n_estimators=5, subsample=1\n",
    "                                  , min_samples_split=2, min_samples_leaf=1, max_depth=3\n",
    "                                  , init=None, random_state=None, max_features=None\n",
    "                                  , verbose=0, max_leaf_nodes=None, warm_start=False\n",
    "                                  )\n",
    "folds = KFold(n_splits=5, shuffle=True, random_state=2018)\n",
    "oof_xgb = np.zeros(len(train))\n",
    "predictions_gbdt = np.zeros(len(test))\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train, y_train)):\n",
    "    print(\"fold n°{}\".format(fold_+1))\n",
    "    trn_data = xgb.DMatrix(X_train[trn_idx], y_train[trn_idx])\n",
    "    val_data = xgb.DMatrix(X_train[val_idx], y_train[val_idx])\n",
    "    \n",
    "    watchlist = [(trn_data, 'train'), (val_data, 'valid_data')]\n",
    "    clf = xgb.train(dtrain=trn_data, num_boost_round=20000, evals=watchlist, early_stopping_rounds=200, verbose_eval=100, params=xgb_params,feval = myFeval)\n",
    "    oof_xgb[val_idx] = clf.predict(xgb.DMatrix(X_train[val_idx]), ntree_limit=clf.best_ntree_limit)\n",
    "    predictions_xgb += clf.predict(xgb.DMatrix(X_test), ntree_limit=clf.best_ntree_limit) / folds.n_splits\n",
    "    \n",
    "print(\"CV score: {:<8.8f}\".format(mean_squared_error(oof_xgb, y_train_)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_feat = np.array([[1, 5, 20],\n",
    "                       [2, 7, 30],\n",
    "                       [3, 21, 70],\n",
    "                       [4, 30, 60],\n",
    "                       ])\n",
    "train_label = np.array([[0], [0], [1], [1]]).ravel()\n",
    "\n",
    "test_feat = np.array([[5, 25, 65]])\n",
    "test_label = np.array([[1]])\n",
    "print(train_feat.shape, train_label.shape, test_feat.shape, test_label.shape)\n",
    "\n",
    "gbdt.fit(train_feat, train_label)\n",
    "pred = gbdt.predict(test_feat)\n",
    "\n",
    "total_err = 0\n",
    "for i in range(pred.shape[0]):\n",
    "    print(pred[i], test_label[i])\n",
    "    err = (pred[i] - test_label[i]) / test_label[i]\n",
    "    total_err += err * err\n",
    "print(total_err / pred.shape[0])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
